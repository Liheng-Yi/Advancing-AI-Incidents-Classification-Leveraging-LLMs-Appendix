Few-Shot Learning Classification Results
```json
[{"id":"1","Country":"Worldwide","State":"","City":"","Continent":"Worldwide","Company":"YouTube","Company city":"San Bruno","Company state":"California","Affected population":"Children, Parents","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence, Mental Health, Other","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"content filtering","Online":"yes","title":"Google’s YouTube Kids App Presents Inappropriate Content","description":"YouTube’s content filtering and recommendation algorithms exposed children to disturbing and inappropriate videos.","date occurrence":"2015-05-19","date publicly known":"2017-03-26"},{"id":"2","Country":"United States","State":"New Jersey","City":"Robbinsville","Continent":"North America","Company":"Amazon","Company city":"Seattle","Company state":"Washington","Affected population":"Warehouse Workers","Number of people actually affected":"24","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"Warehouse automation","Online":"No","title":"Warehouse robot ruptures can of bear spray and injures workers","description":"Twenty-four Amazon workers in New Jersey were hospitalized after a robot punctured a can of bear repellent spray in a warehouse.","date occurrence":"2018-12-05","date publicly known":"2018-12-06"},{"id":"3","Country":"Indonesia","State":"","City":"Jakarta","Continent":"Asia","Company":"Boeing","Company city":"Chicago","Company state":"Illinois","Affected population":"Airline Passengers, Crew Members","Number of people actually affected":"189","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence, Other","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"Flight control systems","Online":"no","title":"Crashes with Maneuvering Characteristics Augmentation System (MCAS)","description":"A Boeing 737 crashed into the sea, killing 189 people, after faulty sensor data caused an automated manuevering system to repeatedly push the plane's nose downward.","date occurrence":"2018-10-27","date publicly known":"2019-03-13"},{"id":"4","Country":"USA","State":"Arizona","City":"Tempe","Continent":"North America","Company":"Uber","Company city":"San Francisco","Company state":"California","Affected population":"Pedestrians, Autonomous Vehicle Users, General Public","Number of people actually affected":"1","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence, Other","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"Autonomous Vehicles","Online":"no","title":"Uber AV Killed Pedestrian in Arizona","description":"An Uber autonomous vehicle (AV) in autonomous mode struck and killed a pedestrian in Tempe, Arizona.","date occurrence":"2018-03-18","date publicly known":"2018-03-22"},{"id":"5","Country":"United States","State":"","City":"","Continent":"North America","Company":"Not specified in the article","Company city":"Not specified in the article","Company state":"","Affected population":"Patients undergoing robotic surgery","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence, Other","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"Robotic surgery","Online":"no","title":"Collection of Robotic Surgery Malfunctions","description":"Study on database reports of robotic surgery malfunctions (8,061), including those ending in injury (1,391) and death (144), between 2000 and 2013.","date occurrence":"2015-07-13","date publicly known":"2015-07-20"},{"id":"6","Country":"Worldwide","State":"","City":"","Continent":"Worldwide","Company":"Microsoft","Company city":"Redmond","Company state":"Washington","Affected population":"Twitter Users, Online Population","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence, Disinformation, Other","Subclasses":"Technical, Textual","Sub-subclass":"","Area of AI Application":"Social Media Interaction","Online":"yes","title":"TayBot","description":"Microsoft's Tay, an artificially intelligent chatbot, was released on March 23, 2016 and removed within 24 hours due to multiple racist, sexist, and anit-semitic tweets generated by the bot.","date occurrence":"2016-03-24","date publicly known":"2019-11-24"},{"id":"9","Country":"United States","State":"New York","City":"New York City","Continent":"North America","Company":"New York City Department of Education","Company city":"New York City","Company state":"New York","Affected population":"Students, Teachers, Parents","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Human Incompetence, Disinformation","Subclasses":"Administrative, Textual","Sub-subclass":"","Area of AI Application":"Educational assessment and teacher evaluation","Online":"yes","title":"NY City School Teacher Evaluation Algorithm Contested","description":"An algorithm used to rate the effectiveness of school teachers in New York has resulted in thousands of disputes of its results.","date occurrence":"2012-02-25","date publicly known":"2013-10-18"},{"id":"10","Country":"United States","State":"Various","City":"Various","Continent":"North America","Company":"Starbucks","Company city":"Seattle","Company state":"Washington","Affected population":"Retail Workers, Single Parents, Students","Number of people actually affected":"Unknown","Number of people potentially affected":"130,000 (Starbucks baristas nationwide)","Classes of irresponsible AI use":"Human Incompetence, Mental Health","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"Workforce management, Employee scheduling","Online":"No","title":"Kronos Scheduling Algorithm Allegedly Caused Financial Issues for Starbucks Employees","description":"Kronos’s scheduling algorithm and its use by Starbucks managers allegedly negatively impacted financial and scheduling stability for Starbucks employees, which disadvantaged wage workers.","date occurrence":"2014-08-14","date publicly known":"2015-06-02"},{"id":"11","Country":"United States","State":"Florida","City":"Fort Lauderdale","Continent":"North America","Company":"Northpointe","Company city":"Canton","Company state":"Ohio","Affected population":"African American, Criminal Defendants","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Discrimination, Human Incompetence","Subclasses":"Data bias, Algorithmic bias, Technical","Sub-subclass":"Race, Feedback loop, Optimization function","Area of AI Application":"criminal justice system, risk assessment","Online":"yes","title":"Northpointe Risk Models","description":"An algorithm developed by Northpointe and used in the penal system is two times more likely to incorrectly label a black person as a high-risk re-offender and is two times more likely to incorrectly label a white person as low-risk for reoffense according to a ProPublica review.","date occurrence":"2016-05-23","date publicly known":"2016-05-22"},{"id":"13","Country":"Worldwide","State":"","City":"","Continent":"Worldwide","Company":"Google","Company city":"Mountain View","Company state":"California","Affected population":"Online Users","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Discrimination, Human Incompetence","Subclasses":"Data bias, Algorithmic bias, Technical","Sub-subclass":"Race, Sexual Orientation, Feedback loop","Area of AI Application":"content moderation","Online":"yes","title":"High-Toxicity Assessed on Text Involving Women and Minority Groups","description":"Google's Perspective API, which assigns a toxicity score to online text, seems to award higher toxicity scores to content involving non-white, male, Christian, heterosexual phrases.","date occurrence":"2017-02-27","date publicly known":"2021-02-09"},{"id":"14","Country":"Worldwide","State":"","City":"","Continent":"Worldwide","Company":"Google","Company city":"Mountain View","Company state":"California","Affected population":"LGBTQ, Religious and Ethnic Minorities","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Discrimination","Subclasses":"Data bias, Algorithmic bias","Sub-subclass":"Gender, Race, Sexual Orientation, Other","Area of AI Application":"Sentiment Analysis","Online":"yes","title":"Biased Sentiment Analysis","description":"Google Cloud's Natural Language API provided racist, homophobic, amd antisemitic sentiment analyses.","date occurrence":"2017-10-26","date publicly known":"2017-10-25"},{"id":"167","Country":"United States","State":"California","City":"Stanford","Continent":"North America","Company":"Stanford University","Company city":"Stanford","Company state":"California","Affected population":"LGBTQ","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Discrimination, Pseudoscience, Mental Health","Subclasses":"Data bias, Facial","Sub-subclass":"Sexual Orientation","Area of AI Application":"Facial recognition analysis","Online":"yes","title":"Researchers' Homosexual-Men Detection Model Denounced as a Threat to LGBTQ People’s Safety and Privacy","description":"Researchers at Stanford Graduate School of Business developed a model that determined, on a binary scale, whether someone was homosexual using only his facial image, which advocacy groups such as GLAAD and the Human Rights Campaign denounced as flawed science and threatening to LGBTQ folks.","date occurrence":"2017-09-07","date publicly known":"2017-10-08"},{"id":"382","Country":"United Kingdom","State":"","City":"London","Continent":"Europe","Company":"Meta","Company city":"Menlo Park","Company state":"California","Affected population":"Online Female Population","Number of people actually affected":"1","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Mental Health, Disinformation","Subclasses":"Textual","Sub-subclass":"","Area of AI Application":"content filtering","Online":"yes","title":"Instagram's Exposure of Harmful Content Contributed to Teenage Girl’s Suicide","description":"Instagram was ruled by a judge to have contributed to the death of a teenage girl in the UK allegedly through its exposure and recommendation of suicide, self-harm, and depressive content.","date occurrence":"2017-11-21","date publicly known":"2022-09-30"},{"id":"451","Country":"United Kingdom","State":"","City":"London","Continent":"Europe","Company":"Stability AI","Company city":"London","Company state":"","Affected population":"Artists, Photographers","Number of people actually affected":"Unknown","Number of people potentially affected":"Thousands—possibly millions","Classes of irresponsible AI use":"Copyright Violation","Subclasses":"Copyright Violation","Sub-subclass":"","Area of AI Application":"AI art generation","Online":"yes","title":"Stable Diffusion's Training Data Contained Copyrighted Images","description":"Stability AI reportedly scraped copyrighted images by Getty Images to be used as training data for Stable Diffusion model.","date occurrence":"2022-10-16","date publicly known":"2023-01-16"},{"id":"505","Country":"Belgium","State":"","City":"","Continent":"Europe","Company":"Chai Research","Company city":"Silicon Valley","Company state":"California","Affected population":"Online Users","Number of people actually affected":"1","Number of people potentially affected":"Millions","Classes of irresponsible AI use":"Mental Health, Human Incompetence, Other","Subclasses":"Technical","Sub-subclass":"","Area of AI Application":"Conversational AI","Online":"yes","title":"Man Reportedly Committed Suicide Following Conversation with Chai Chatbot","description":"A Belgian man reportedly committed suicide following a conversation with Eliza, a language model developed by Chai that encouraged the man to commit suicide to improve the health of the planet.","date occurrence":"2023-03-27","date publicly known":"2023-03-27"},{"id":"39","Country":"Worldwide","State":"","City":"","Continent":"Worldwide","Company":"Multiple companies including Adobe, University of Washington, Lyrebird, and others involved in AI research and development","Company city":"Multiple locations","Company state":"","Affected population":"General Public","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Disinformation, Mental Health","Subclasses":"Video","Sub-subclass":"","Area of AI Application":"Video and audio manipulation, content creation","Online":"yes","title":"Deepfake Obama Introduction of Deepfakes","description":"University of Washington researchers made a deepfake of Obama, followed by Jordan Peele","date occurrence":"2017-07-01","date publicly known":"2017-07-18"},{"id":"0","Country":"Worldwide","State":"","City":"","Continent":"Worldwide","Company":"Not specified","Company city":"Not specified","Company state":"","Affected population":"Global Population","Number of people actually affected":"Unknown","Number of people potentially affected":"Unknown","Classes of irresponsible AI use":"Environmental Impact","Subclasses":"","Sub-subclass":"","Area of AI Application":"General AI development and application","Online":"yes","title":"AI's Environmental Footprint","description":"The environmental footprint of AI, particularly in training large models, is significant. According to a study by researchers at the University of Massachusetts, the energy used in training certain popular large AI models can produce about 626,000 pounds of carbon dioxide. This amount is equivalent to roughly 300 round-trip flights between New York and San Francisco, highlighting the substantial carbon footprint associated with advanced AI technologies. This data underscores the need for more sustainable practices in the field of AI to mitigate its impact on climate change.","date occurrence":"2023-07-18","date publicly known":"2023-07-18"}]```
